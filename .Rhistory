qbeta(c(0.05, 0.95), a + s, b + f)
ps = rbeta(1000, a + s, b + f)
hist(ps,xlab="p")
sum(ps >= 0.5)/1000
quantile(ps, c(0.05, 0.95))
#####################################
# Section 2.5 Using a Histogram Prior
#####################################
library(LearnBayes)
midpt = seq(0.05, 0.95, by = 0.1)
prior = c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)
prior = prior/sum(prior)
curve(histprior(x,midpt,prior), from=0, to=1,
ylab="Prior density",ylim=c(0,.3))
s = 11
f = 16
curve(histprior(x,midpt,prior) * dbeta(x,s+1,f+1),
from=0, to=1, ylab="Posterior density")
p = seq(0, 1, length=500)
post = histprior(p, midpt, prior) *
dbeta(p, s+1, f+1)
post = post/sum(post)
ps = sample(p, replace = TRUE, prob = post)
hist(ps, xlab="p", main="")
########################
# Section 2.6 Prediction
########################
library(LearnBayes)
p=seq(0.05, 0.95, by=.1)
prior = c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)
prior=prior/sum(prior)
m=20; ys=0:20
pred=pdiscp(p, prior, m, ys)
cbind(0:20,pred)
ab=c(3.26, 7.19)
m=20; ys=0:20
pred=pbetap(ab, m, ys)
p=rbeta(1000,3.26, 7.19)
y = rbinom(1000, 20, p)
table(y)
freq=table(y)
ys=as.integer(names(freq))
predprob=freq/sum(freq)
plot(ys,predprob,type="h",xlab="y",
ylab="Predictive Probability")
dist=cbind(ys,predprob)
covprob=.9
discint(dist,covprob)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(LearnBayes)
####################################################
# Section 6.2 Introduction to Discrete Markov Chains
####################################################
P=matrix(c(.5,.5,0,0,0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,
0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,0,0,.5,.5),
nrow=6,ncol=6,byrow=TRUE)
P
s=array(0,c(50000,1))
s[1]=3
for (j in 2:50000)
s[j]=sample(1:6,size=1,prob=P[s[j-1],])
m=c(500,2000,8000,50000)
for (i in 1:4)
print(table(s[1:m[i]])/m[i])
w=matrix(c(.1,.2,.2,.2,.2,.1),nrow=1,ncol=6)
w%*%P
##################################################################
# Section 6.7 Learning about a Normal Population from Grouped Data
##################################################################
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
y=c(rep(65,14),rep(67,30),rep(69,49),rep(71,70),rep(73,33),
rep(75,15))
mean(y)
log(sd(y))
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
fit
modal.sds=sqrt(diag(fit$var))
proposal=list(var=fit$var,scale=2)
fit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
fit2$accept
post.means=apply(fit2$par,2,mean)
post.sds=apply(fit2$par,2,sd)
cbind(c(fit$mode),modal.sds)
cbind(post.means,post.sds)
mycontour(groupeddatapost,c(69,71,.6,1.3),d,
xlab="mu",ylab="log sigma")
points(fit2$par[5001:10000,1],fit2$par[5001:10000,2])
##################################################
# Section 6.8 Example of Output Analysis
##################################################
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
library(coda)
library(lattice)
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
start=c(65,1)
proposal=list(var=fit$var,scale=0.2)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit$par[-c(1:2000),]),col="black")
S=readline(prompt="Type  <Return>   to continue : ")
windows()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(LearnBayes)
####################################################
# Section 6.2 Introduction to Discrete Markov Chains
####################################################
P=matrix(c(.5,.5,0,0,0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,
0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,0,0,.5,.5),
nrow=6,ncol=6,byrow=TRUE)
P
s=array(0,c(50000,1))
s[1]=3
for (j in 2:50000)
s[j]=sample(1:6,size=1,prob=P[s[j-1],])
m=c(500,2000,8000,50000)
for (i in 1:4)
print(table(s[1:m[i]])/m[i])
w=matrix(c(.1,.2,.2,.2,.2,.1),nrow=1,ncol=6)
w%*%P
##################################################################
# Section 6.7 Learning about a Normal Population from Grouped Data
##################################################################
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
y=c(rep(65,14),rep(67,30),rep(69,49),rep(71,70),rep(73,33),
rep(75,15))
mean(y)
log(sd(y))
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
fit
modal.sds=sqrt(diag(fit$var))
proposal=list(var=fit$var,scale=2)
fit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
fit2$accept
post.means=apply(fit2$par,2,mean)
post.sds=apply(fit2$par,2,sd)
cbind(c(fit$mode),modal.sds)
cbind(post.means,post.sds)
mycontour(groupeddatapost,c(69,71,.6,1.3),d,
xlab="mu",ylab="log sigma")
points(fit2$par[5001:10000,1],fit2$par[5001:10000,2])
##################################################
# Section 6.8 Example of Output Analysis
##################################################
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
library(coda)
library(lattice)
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
start=c(65,1)
proposal=list(var=fit$var,scale=0.2)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit$par[-c(1:2000),]),col="black")
par(mfrow=c(2,1))
autocorr.plot(mcmc(bayesfit$par[-c(1:2000),]),auto.layout=FALSE)
summary(mcmc(bayesfit$par[-c(1:2000),]))
batchSE(mcmc(bayesfit$par[-c(1:2000),]), batchSize=50)
start=c(70,1)
proposal=list(var=fit$var,scale=2.0)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
sim.parameters=mcmc(bayesfit$par[-c(1:2000),])
xyplot(mcmc(bayesfit$par[-c(1:2000),]),col="black")
par(mfrow=c(2,1))
autocorr.plot(sim.parameters,auto.layout=FALSE)
summary(sim.parameters)
batchSE(sim.parameters, batchSize=50)
###################################################
# Section 6.9 Modeling Data with Cauchy Errors
###################################################
library(LearnBayes)
data(darwin)
attach(darwin)
mean(difference)
log(sd(difference))
laplace(cauchyerrorpost,c(21.6,3.6),difference)
laplace(cauchyerrorpost,.1*c(21.6,3.6),difference)$mode
c(24.7-4*sqrt(34.96),24.7+4*sqrt(34.96))
c(2.77-4*sqrt(.138),2.77+4*sqrt(.138))
mycontour(cauchyerrorpost,c(-10,60,1,4.5),difference,
xlab="mu",ylab="log sigma")
fitlaplace=laplace(cauchyerrorpost,c(21.6,3.6), difference)
mycontour(lbinorm,c(-10,60,1,4.5),list(m=fitlaplace$mode,
v=fitlaplace$var), xlab="mu",ylab="log sigma")
proposal=list(var=fitlaplace$var,scale=2.5)
start=c(20,3)
m=1000
s=rwmetrop(cauchyerrorpost,proposal,start,m,difference)
mycontour(cauchyerrorpost,c(-10,60,1,4.5),difference,
xlab="mu",ylab="log sigma")
points(s$par[,1],s$par[,2])
fitgrid=simcontour(cauchyerrorpost,c(-10,60,1,4.5),difference,
50000)
proposal=list(var=fitlaplace$var,scale=2.5)
start=c(20,3)
fitrw=rwmetrop(cauchyerrorpost,proposal,start,50000,
difference)
proposal2=list(var=fitlaplace$var,mu=t(fitlaplace$mode))
fitindep=indepmetrop(cauchyerrorpost,proposal2,start,50000,
difference)
fitgibbs=gibbs(cauchyerrorpost,start,50000,c(12,.75),
difference)
apply(fitrw$par,2,mean)
apply(fitrw$par,2,sd)
#############################################################
# Section 6.10 Analysis of the Stanford Heart Transplant Data
#############################################################
library(LearnBayes)
data(stanfordheart)
start=c(0,3,-1)
laplacefit=laplace(transplantpost,start,stanfordheart)
laplacefit
proposal=list(var=laplacefit$var,scale=2)
s=rwmetrop(transplantpost,proposal,start,10000,stanfordheart)
s$accept
par(mfrow=c(2,2))
tau=exp(s$par[,1])
plot(density(tau),main="TAU")
lambda=exp(s$par[,2])
plot(density(lambda),main="LAMBDA")
p=exp(s$par[,3])
plot(density(p),main="P")
apply(exp(s$par),2,quantile,c(.05,.5,.95))
par(mfrow=c(1,1))
t=seq(1,240)
p5=0*t; p50=0*t; p95=0*t
for (j in 1:240)
{ S=(lambda/(lambda+t[j]))^p
q=quantile(S,c(.05,.5,.95))
p5[j]=q[1]; p50[j]=q[2]; p95[j]=q[3]}
plot(t,p50,type="l",ylim=c(0,1),ylab="Prob(Survival)",
xlab="time")
lines(t,p5,lty=2)
lines(t,p95,lty=2)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(LearnBayes)
####################################################
# Section 6.2 Introduction to Discrete Markov Chains
####################################################
P=matrix(c(.5,.5,0,0,0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,
0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,0,0,.5,.5),
nrow=6,ncol=6,byrow=TRUE)
P
s=array(0,c(50000,1))
s[1]=3
for (j in 2:50000)
s[j]=sample(1:6,size=1,prob=P[s[j-1],])
m=c(500,2000,8000,50000)
for (i in 1:4)
print(table(s[1:m[i]])/m[i])
w=matrix(c(.1,.2,.2,.2,.2,.1),nrow=1,ncol=6)
w%*%P
##################################################################
# Section 6.7 Learning about a Normal Population from Grouped Data
##################################################################
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
y=c(rep(65,14),rep(67,30),rep(69,49),rep(71,70),rep(73,33),
rep(75,15))
mean(y)
log(sd(y))
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
fit
modal.sds=sqrt(diag(fit$var))
proposal=list(var=fit$var,scale=2)
fit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
fit2$accept
post.means=apply(fit2$par,2,mean)
post.sds=apply(fit2$par,2,sd)
cbind(c(fit$mode),modal.sds)
cbind(post.means,post.sds)
mycontour(groupeddatapost,c(69,71,.6,1.3),d,
xlab="mu",ylab="log sigma")
points(fit2$par[5001:10000,1],fit2$par[5001:10000,2])
##################################################
# Section 6.8 Example of Output Analysis
##################################################
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
library(coda)
library(lattice)
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
start=c(65,1)
proposal=list(var=fit$var,scale=0.2)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit$par[-c(1:2000),]),col="black")
par(mfrow=c(2,1))
autocorr.plot(mcmc(bayesfit$par[-c(1:2000),]),auto.layout=FALSE)
summary(mcmc(bayesfit$par[-c(1:2000),]))
batchSE(mcmc(bayesfit$par[-c(1:2000),]), batchSize=50)
start=c(70,1)
proposal=list(var=fit$var,scale=2.0)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
sim.parameters=mcmc(bayesfit$par[-c(1:2000),])
xyplot(mcmc(bayesfit$par[-c(1:2000),]),col="black")
par(mfrow=c(2,1))
autocorr.plot(sim.parameters,auto.layout=FALSE)
summary(sim.parameters)
batchSE(sim.parameters, batchSize=50)
###################################################
# Section 6.9 Modeling Data with Cauchy Errors
###################################################
library(LearnBayes)
data(darwin)
attach(darwin)
mean(difference)
log(sd(difference))
laplace(cauchyerrorpost,c(21.6,3.6),difference)
laplace(cauchyerrorpost,.1*c(21.6,3.6),difference)$mode
c(24.7-4*sqrt(34.96),24.7+4*sqrt(34.96))
c(2.77-4*sqrt(.138),2.77+4*sqrt(.138))
mycontour(cauchyerrorpost,c(-10,60,1,4.5),difference,
xlab="mu",ylab="log sigma")
fitlaplace=laplace(cauchyerrorpost,c(21.6,3.6), difference)
mycontour(lbinorm,c(-10,60,1,4.5),list(m=fitlaplace$mode,
v=fitlaplace$var), xlab="mu",ylab="log sigma")
proposal=list(var=fitlaplace$var,scale=2.5)
start=c(20,3)
m=1000
s=rwmetrop(cauchyerrorpost,proposal,start,m,difference)
mycontour(cauchyerrorpost,c(-10,60,1,4.5),difference,
xlab="mu",ylab="log sigma")
points(s$par[,1],s$par[,2])
fitgrid=simcontour(cauchyerrorpost,c(-10,60,1,4.5),difference,
50000)
proposal=list(var=fitlaplace$var,scale=2.5)
start=c(20,3)
fitrw=rwmetrop(cauchyerrorpost,proposal,start,50000,
difference)
proposal2=list(var=fitlaplace$var,mu=t(fitlaplace$mode))
fitindep=indepmetrop(cauchyerrorpost,proposal2,start,50000,
difference)
fitgibbs=gibbs(cauchyerrorpost,start,50000,c(12,.75),
difference)
apply(fitrw$par,2,mean)
apply(fitrw$par,2,sd)
#############################################################
# Section 6.10 Analysis of the Stanford Heart Transplant Data
#############################################################
library(LearnBayes)
data(stanfordheart)
start=c(0,3,-1)
laplacefit=laplace(transplantpost,start,stanfordheart)
laplacefit
proposal=list(var=laplacefit$var,scale=2)
s=rwmetrop(transplantpost,proposal,start,10000,stanfordheart)
s$accept
par(mfrow=c(2,2))
tau=exp(s$par[,1])
plot(density(tau),main="TAU")
lambda=exp(s$par[,2])
plot(density(lambda),main="LAMBDA")
p=exp(s$par[,3])
plot(density(p),main="P")
apply(exp(s$par),2,quantile,c(.05,.5,.95))
par(mfrow=c(1,1))
t=seq(1,240)
p5=0*t; p50=0*t; p95=0*t
for (j in 1:240)
{ S=(lambda/(lambda+t[j]))^p
q=quantile(S,c(.05,.5,.95))
p5[j]=q[1]; p50[j]=q[2]; p95[j]=q[3]}
plot(t,p50,type="l",ylim=c(0,1),ylab="Prob(Survival)",
xlab="time")
lines(t,p5,lty=2)
lines(t,p95,lty=2)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(LearnBayes)
################################
# Section 9.2.6 An Example
################################
library(LearnBayes)
data(birdextinct)
attach(birdextinct)
logtime=log(time)
plot(nesting,logtime)
out = (logtime > 3)
text(nesting[out], logtime[out], label=species[out], pos = 2)
plot(jitter(size),logtime,xaxp=c(0,1,1))
plot(jitter(status),logtime,xaxp=c(0,1,1))
##### Least-squares fit
fit=lm(logtime~nesting+size+status,data=birdextinct,x=TRUE,y=TRUE)
summary(fit)
##### Sampling from posterior
theta.sample=blinreg(fit$y,fit$x,5000)
par(mfrow=c(2,2))
hist(theta.sample$beta[,2],main="NESTING",
xlab=expression(beta[1]))
hist(theta.sample$beta[,3],main="SIZE",
xlab=expression(beta[2]))
hist(theta.sample$beta[,4],main="STATUS",
xlab=expression(beta[3]))
hist(theta.sample$sigma,main="ERROR SD",
xlab=expression(sigma))
apply(theta.sample$beta,2,quantile,c(.05,.5,.95))
quantile(theta.sample$sigma,c(.05,.5,.95))
###### Estimating mean extinction times
cov1=c(1,4,0,0)
cov2=c(1,4,1,0)
cov3=c(1,4,0,1)
cov4=c(1,4,1,1)
X1=rbind(cov1,cov2,cov3,cov4)
mean.draws=blinregexpected(X1,theta.sample)
c.labels=c("A","B","C","D")
par(mfrow=c(2,2))
for (j in 1:4)
hist(mean.draws[,j],
main=paste("Covariate set",c.labels[j]),xlab="log TIME")
######## Predicting extinction times
cov1=c(1,4,0,0)
cov2=c(1,4,1,0)
cov3=c(1,4,0,1)
cov4=c(1,4,1,1)
X1=rbind(cov1,cov2,cov3,cov4)
pred.draws=blinregpred(X1,theta.sample)
c.labels=c("A","B","C","D")
par(mfrow=c(2,2))
for (j in 1:4)
hist(pred.draws[,j],
main=paste("Covariate set",c.labels[j]),xlab="log TIME")
######### Model checking via posterior predictive distribution
pred.draws=blinregpred(fit$x,theta.sample)
pred.sum=apply(pred.draws,2,quantile,c(.05,.95))
par(mfrow=c(1,1))
ind=1:length(logtime)
matplot(rbind(ind,ind),pred.sum,type="l",lty=1,col=1,
xlab="INDEX",ylab="log TIME")
points(ind,logtime,pch=19)
out=(logtime>pred.sum[2,])
text(ind[out], logtime[out], label=species[out], pos = 4)
######### Model checking via bayes residuals
prob.out=bayesresiduals(fit,theta.sample,2)
par(mfrow=c(1,1))
plot(nesting,prob.out)
out = (prob.out > 0.35)
text(nesting[out], prob.out[out], label=species[out], pos = 4)
##############################################
# Section 9.3 Modeling Using Zellner's g Prior
##############################################
library(LearnBayes)
# illustrating the role of the parameter c
data(puffin)
X=cbind(1, puffin$Distance - mean(puffin$Distance))
c.prior=c(0.1,0.5,5,2)
fit=vector("list",4)
for (j in 1:4)
{
prior=list(b0=c(8,0), c0=c.prior[j])
fit[[j]]=blinreg(puffin$Nest, X, 1000, prior)
}
BETA=NULL
for (j in 1:4)
{
s=data.frame(Prior=paste("c =",as.character(c.prior[j])),
beta0=fit[[j]]$beta[,1],beta1=fit[[j]]$beta[,2])
BETA=rbind(BETA,s)
}
library(lattice)
with(BETA,xyplot(beta1~beta0|Prior,type=c("p","g"),col="black"))
# model selection
data=list(y=puffin$Nest, X=cbind(1,puffin$Grass,puffin$Soil))
prior=list(b0=c(0,0,0), c0=100)
beta.start=with(puffin,lm(Nest~Grass+Soil)$coef)
laplace(reg.gprior.post,c(beta.start,0),list(data=data,prior=prior))$int
X=puffin[,-1]; y=puffin$Nest; c=100
bayes.model.selection(y,X,c,constant=FALSE)
##############################################
# Section 9.4 Survival Modeling
##############################################
library(LearnBayes)
data(chemotherapy)
attach(chemotherapy)
library(survival)
survreg(Surv(time,status)~factor(treat)+age,dist="weibull")
start=c(-.5,9,.5,-.05)
d=cbind(time,status,treat-1,age)
fit=laplace(weibullregpost,start,d)
fit
proposal=list(var=fit$var,scale=1.5)
bayesfit=rwmetrop(weibullregpost,proposal,fit$mode,10000,d)
bayesfit$accept
par(mfrow=c(2,2))
sigma=exp(bayesfit$par[,1])
mu=bayesfit$par[,2]
beta1=bayesfit$par[,3]
beta2=bayesfit$par[,4]
hist(beta1,xlab="treatment",main="")
hist(beta2,xlab="age",main="")
hist(sigma,xlab="sigma",main="")
